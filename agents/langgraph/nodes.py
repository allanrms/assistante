"""
N√≥s do grafo LangGraph para Secret√°ria Virtual

Cada n√≥ √© uma fun√ß√£o que recebe o estado e retorna atualiza√ß√µes.

PRINC√çPIOS:
- LangGraph controla o fluxo (n√£o AgentExecutor)
- LLM apenas classifica inten√ß√£o ou gera texto
- Ferramentas s√≥ chamadas por n√≥s espec√≠ficos
- Conversation.status √© autoridade m√°xima
- Transfer√™ncia humana √© estado terminal
"""

from langgraph.graph import END
from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI

from .state import SecretaryState
from .runtime import SecretaryRuntime
from .tools import (
    gerar_link_agendamento,
    consultar_agendamentos,
    cancelar_agendamento,
    reagendar_consulta,
    request_human_intervention
)
from agents.models import Agent, Conversation
from agents.patterns.factories.llm_factory import LLMFactory


# ==============================================================================
# N√ì 1: CONVERSATION GUARD
# ==============================================================================

def conversation_guard(state: SecretaryState):
    """
    Bloqueia respostas da IA se o atendimento for humano.

    Este √© o primeiro n√≥ executado. Marca no estado se pode continuar.

    Args:
        state: Estado atual do grafo

    Returns:
        dict: Estado com flag 'can_continue'
    """
    conversation = state.conversation

    # Print cabe√ßalho da mensagem
    print("\n" + "="*80)
    print(f"üì® NOVA MENSAGEM | Conversa #{conversation.id}")
    print(f"üë§ USU√ÅRIO: {state.user_input}")
    print("="*80)

    if conversation.status != "ai":
        print(f"üõë GUARD: Bloqueado (status: {conversation.status})")
        # Marca que n√£o pode continuar
        return {"intent": "BLOCKED"}

    print(f"‚úÖ GUARD: Liberado")
    return state


# ==============================================================================
# N√ì 2: SAUDA√á√ÉO INICIAL
# ==============================================================================

def initial_greeting(state: SecretaryState):
    """
    Responde com sauda√ß√£o inicial ao usu√°rio usando LLM.

    Este n√≥ √© executado para toda mensagem recebida, fornecendo
    uma resposta amig√°vel e personalizada antes de processar a inten√ß√£o.

    Args:
        state: Estado atual do grafo

    Returns:
        dict: Estado com mensagem de sauda√ß√£o enviada
    """
    conversation = state.conversation
    message = state.message
    agent = state.agent

    # Verificar se √© a primeira mensagem da conversa
    total_messages = conversation.messages.count()
    is_first_message = total_messages <= 1

    # Usar LLM para gerar sauda√ß√£o personalizada
    llm = LLMFactory(agent).llm
    base_prompt = agent.build_prompt()

    response = llm.invoke(base_prompt)
    greeting = response.content.strip()

    # Enviar sauda√ß√£o
    runtime = SecretaryRuntime(
        conversation=conversation,
        channel=state.channel,
        messages_buffer=state.messages_sent
    )
    runtime.send_message(greeting)

    print(f"üëã Sauda√ß√£o enviada (primeira mensagem: {is_first_message})")

    return state


# ==============================================================================
# N√ì 3: DETEC√á√ÉO DE INTEN√á√ÉO
# ==============================================================================

def detect_intent(state: SecretaryState):
    """
    Detecta a inten√ß√£o do usu√°rio usando LLM com contexto do hist√≥rico.

    Inten√ß√µes v√°lidas:
    - AGENDAR: usu√°rio quer criar novo agendamento
    - CONSULTAR: usu√°rio quer ver agendamentos existentes
    - CANCELAR: usu√°rio quer cancelar agendamento
    - REAGENDAR: usu√°rio quer mudar data/hora de agendamento
    - HUMANO: usu√°rio pede para falar com atendente
    - OUTRO: qualquer outra coisa

    Args:
        state: Estado atual do grafo

    Returns:
        dict: Atualiza√ß√£o do estado com 'intent' preenchido
    """
    # Verificar se estamos em um fluxo de continua√ß√£o (aguardando dados/ID)
    # Analisa se a PEN√öLTIMA mensagem foi da IA pedindo algo e a √öLTIMA foi do usu√°rio respondendo
    chat_history = state.chat_history

    if len(chat_history) >= 2:
        # Pegar as duas √∫ltimas mensagens
        last_message = chat_history[-1] if chat_history else None
        second_last_message = chat_history[-2] if len(chat_history) >= 2 else None

        # Verificar se pen√∫ltima √© AI e √∫ltima √© HUMAN (usu√°rio respondeu)
        if (second_last_message and hasattr(second_last_message, 'type') and second_last_message.type == 'ai' and
            last_message and hasattr(last_message, 'type') and last_message.type == 'human'):

            ai_message = second_last_message.content

            # Se IA pediu ID para cancelar
            if "Informe o ID da consulta que deseja cancelar" in ai_message:
                print("üîÑ Continuando fluxo de cancelamento (aguardando ID)")
                return {"intent": "CANCELAR", "step": "AGUARDANDO_ID_CANCELAR"}

            # Se IA pediu ID para reagendar
            if "Informe o ID da consulta que deseja reagendar" in ai_message:
                print("üîÑ Continuando fluxo de reagendamento (aguardando ID)")
                return {"intent": "REAGENDAR", "step": "AGUARDANDO_ID_REAGENDAR"}

    # Usar agente do estado
    agent = state.agent

    llm = LLMFactory(agent).llm
    # Carregar prompt do sistema configurado no Agent model
    base_prompt = agent.build_prompt()

    # Construir contexto do hist√≥rico (√∫ltimas 3 intera√ß√µes)
    history_context = ""
    if chat_history:
        recent_history = chat_history[-6:]  # √öltimas 3 intera√ß√µes (user + ai)
        history_lines = []
        for msg in recent_history:
            if hasattr(msg, 'type'):
                if msg.type == 'human':
                    history_lines.append(f"Usu√°rio: {msg.content}")
                elif msg.type == 'ai':
                    history_lines.append(f"Assistente: {msg.content}")
        if history_lines:
            history_context = "\n\nHist√≥rico recente:\n" + "\n".join(history_lines)

    # Incluir prompt do sistema se existir
    system_context = ""
    if base_prompt:
        system_context = f"\n\nContexto do sistema:\n{base_prompt}\n"

    # Prompt de classifica√ß√£o
    prompt = f"""{system_context}Classifique a inten√ß√£o do usu√°rio em UMA palavra:

AGENDAR - Usu√°rio quer criar um novo agendamento
CONSULTAR - Usu√°rio quer ver seus agendamentos
CANCELAR - Usu√°rio quer cancelar um agendamento
REAGENDAR - Usu√°rio quer mudar data/hora de agendamento
HUMANO - Usu√°rio pede para falar com atendente humano
OUTRO - Qualquer outra coisa{history_context}

Mensagem atual do usu√°rio:
{state.user_input}

Responda APENAS com uma das palavras acima, nada mais."""

    try:
        response = llm.invoke(prompt)
        intent = response.content.strip().upper()

        # Validar inten√ß√£o
        valid_intents = ['AGENDAR', 'CONSULTAR', 'CANCELAR', 'REAGENDAR', 'HUMANO', 'OUTRO']
        if intent not in valid_intents:
            intent = 'OUTRO'

        print(f"üéØ INTEN√á√ÉO: {intent}")
        return {"intent": intent}

    except Exception as e:
        print(f"‚ùå Erro ao detectar inten√ß√£o: {e}")
        return {"intent": "OUTRO"}


# ==============================================================================
# N√ì 4: TRANSFER√äNCIA HUMANA (ESTADO TERMINAL)
# ==============================================================================

def transfer_to_human(state: SecretaryState):
    """
    Transfere atendimento para humano e ENCERRA o fluxo.

    Este √© um estado terminal. O grafo tem uma edge direta deste n√≥ para END.

    Args:
        state: Estado atual do grafo

    Returns:
        dict: Estado inalterado
    """
    runtime = SecretaryRuntime(state.conversation, state.channel, state.messages_sent)

    # Enviar mensagem de transfer√™ncia
    runtime.send_message(
        "Vou transferir voc√™ para um atendente humano agora. "
        "Aguarde que algu√©m ir√° te responder em breve! üë§"
    )

    # Solicitar interven√ß√£o humana (altera status da conversa)
    request_human_intervention(
        reason="Usu√°rio solicitou atendimento humano",
        runtime=runtime
    )

    print("üö® Transferido para atendimento humano")

    # Print da resposta formatado
    print(f"\nüí¨ RESPOSTA:")
    print("‚îÄ" * 80)
    print("Vou transferir voc√™ para um atendente humano agora. Aguarde que algu√©m ir√° te responder em breve! üë§")
    print("‚îÄ" * 80 + "\n")

    # Retornar estado (edge para END est√° definida no grafo)
    return state


# ==============================================================================
# N√ì 5: VALIDAR DADOS DE AGENDAMENTO
# ==============================================================================

def validar_dados_agendamento(state: SecretaryState):
    """
    Valida se todos os dados necess√°rios para agendamento foram fornecidos.

    Dados obrigat√≥rios:
    - Tipo de atendimento (particular ou conv√™nio)
    - Nome completo do paciente

    Args:
        state: Estado atual do grafo

    Returns:
        dict: Atualiza√ß√£o com dados extra√≠dos e status de valida√ß√£o
    """
    agent = state.agent
    llm = LLMFactory(agent).llm

    # Construir hist√≥rico
    history_messages = []
    for msg in state.chat_history:
        if hasattr(msg, 'type'):
            if msg.type == 'human':
                history_messages.append(f"Usu√°rio: {msg.content}")
            elif msg.type == 'ai':
                history_messages.append(f"Assistente: {msg.content}")

    history_text = "\n".join(history_messages) if history_messages else ""

    # Prompt para extrair dados
    extraction_prompt = f"""Analise o hist√≥rico da conversa e a mensagem atual para extrair os dados de agendamento.

Hist√≥rico:
{history_text}

Mensagem atual: {state.user_input}

Extraia:
1. Tipo de atendimento: "particular" ou "conv√™nio"
2. Nome completo do paciente

Responda APENAS em JSON:
{{"tipo": "particular/conv√™nio/null", "nome_completo": "nome/null"}}"""

    try:
        response = llm.invoke(extraction_prompt)
        import json
        import re

        # Extrair JSON
        json_match = re.search(r'\{.*\}', response.content, re.DOTALL)
        if json_match:
            dados = json.loads(json_match.group())
        else:
            dados = json.loads(response.content.strip())

        tipo = dados.get("tipo")
        nome_completo = dados.get("nome_completo")

        # Validar se dados est√£o completos
        dados_completos = (
            tipo and tipo != "null" and tipo.lower() in ["particular", "conv√™nio", "convenio"] and
            nome_completo and nome_completo != "null" and len(nome_completo) > 3
        )

        if dados_completos:
            print(f"‚úÖ Dados completos - Tipo: {tipo}, Nome: {nome_completo}")
            return {
                "step": "COMPLETO",
                "response": json.dumps({"tipo": tipo, "nome_completo": nome_completo})  # Tempor√°rio
            }
        else:
            print(f"‚ö†Ô∏è Dados incompletos - Tipo: {tipo}, Nome: {nome_completo}")
            return {
                "step": "INCOMPLETO",
                "response": json.dumps({"tipo": tipo, "nome_completo": nome_completo})  # Tempor√°rio
            }

    except Exception as e:
        print(f"‚ùå Erro ao validar dados: {e}")
        return {"step": "INCOMPLETO", "response": "{}"}


# ==============================================================================
# N√ì 6: GERAR LINK DE AGENDAMENTO
# ==============================================================================

def gerar_link(state: SecretaryState):
    """
    Gera link de agendamento quando todos os dados est√£o completos.

    Args:
        state: Estado atual do grafo

    Returns:
        dict: Atualiza√ß√£o com 'response' contendo o link
    """
    runtime = SecretaryRuntime(state.conversation, state.channel, state.messages_sent)

    # Extrair dados do response tempor√°rio
    try:
        import json
        dados = json.loads(state.response)
        tipo = dados.get("tipo", "")
        nome_completo = dados.get("nome_completo", "")
    except:
        tipo = ""
        nome_completo = ""

    # Gerar link
    link = gerar_link_agendamento(runtime)

    # Formatar resposta
    tipo_formatado = tipo.capitalize()
    response_text = f"""‚úÖ Perfeito! Dados confirmados:

üë§ **Paciente:** {nome_completo}
üí≥ **Tipo:** {tipo_formatado}

{link}"""

    print(f"üìÖ Link de agendamento gerado")

    return {"response": response_text}


# ==============================================================================
# N√ì 7: SOLICITAR DADOS FALTANTES
# ==============================================================================

def solicitar_dados(state: SecretaryState):
    """
    Solicita os dados faltantes para o agendamento de forma natural.

    Args:
        state: Estado atual do grafo

    Returns:
        dict: Atualiza√ß√£o com 'response' solicitando dados
    """
    agent = state.agent
    llm = LLMFactory(agent).llm
    base_prompt = agent.build_prompt()

    # Extrair dados parciais
    try:
        import json
        dados = json.loads(state.response)
        tem_tipo = dados.get("tipo") and dados.get("tipo") != "null"
        tem_nome = dados.get("nome_completo") and dados.get("nome_completo") != "null"
    except:
        tem_tipo = False
        tem_nome = False

    # Construir hist√≥rico
    history_messages = []
    for msg in state.chat_history:
        if hasattr(msg, 'type'):
            if msg.type == 'human':
                history_messages.append(f"Usu√°rio: {msg.content}")
            elif msg.type == 'ai':
                history_messages.append(f"Assistente: {msg.content}")

    history_text = "\n".join(history_messages) if history_messages else ""

    # Prompt para solicitar dados de forma natural
    request_prompt = f"""{base_prompt}

---

Hist√≥rico:
{history_text}

Mensagem atual: {state.user_input}

---

O usu√°rio quer agendar uma consulta. Voc√™ precisa coletar:
- Tipo de atendimento: {"‚úì J√Å TEM" if tem_tipo else "‚úó FALTANDO"}
- Nome completo: {"‚úì J√Å TEM" if tem_nome else "‚úó FALTANDO"}

Pe√ßa de forma NATURAL e AMIG√ÅVEL apenas os dados que est√£o faltando.
Seja breve (m√°ximo 2-3 linhas)."""

    try:
        response = llm.invoke(request_prompt)
        response_text = response.content.strip()
        print(f"üìã Solicitando dados faltantes")
        return {"response": response_text}

    except Exception as e:
        print(f"‚ùå Erro ao solicitar dados: {e}")
        # Fallback
        response_text = """Para realizar seu agendamento, preciso de:

üìã Tipo de atendimento (Particular ou Conv√™nio) e Nome completo"""
        return {"response": response_text}


# ==============================================================================
# N√ì 6: CONSULTA DE AGENDAMENTOS
# ==============================================================================

def consultar(state: SecretaryState):
    """
    Lista os agendamentos do contato.

    Args:
        state: Estado atual do grafo

    Returns:
        dict: Atualiza√ß√£o com 'response' contendo a lista
    """
    runtime = SecretaryRuntime(state.conversation, state.channel, state.messages_sent)

    response_text = consultar_agendamentos(runtime)

    print(f"üìã Agendamentos consultados")

    return {"response": response_text}


# ==============================================================================
# N√ì 7: CANCELAMENTO (ETAPA 1 - LISTAR)
# ==============================================================================

def cancelar_listar(state: SecretaryState):
    """
    Lista agendamentos e solicita ID para cancelar.

    Este √© o primeiro passo do fluxo de cancelamento.
    Atualiza 'step' para AGUARDANDO_ID_CANCELAR.

    Args:
        state: Estado atual do grafo

    Returns:
        dict: Atualiza√ß√£o com 'response' e 'step'
    """
    runtime = SecretaryRuntime(state.conversation, state.channel, state.messages_sent)

    # Consultar agendamentos
    result = consultar_agendamentos(runtime)

    # Verificar se h√° agendamentos (detectar se a mensagem indica aus√™ncia)
    if "n√£o possui agendamentos" in result.lower() or "nenhum agendamento" in result.lower():
        # N√£o h√° agendamentos, apenas retornar a mensagem
        print(f"‚ö†Ô∏è Sem agendamentos para cancelar")
        return {"response": result}

    # H√° agendamentos, adicionar instru√ß√£o para solicitar ID
    response_text = result + "\n\n‚ùì Informe o ID da consulta que deseja cancelar."

    print(f"üóëÔ∏è Aguardando ID para cancelamento")

    return {
        "response": response_text,
        "step": "AGUARDANDO_ID_CANCELAR"
    }


# ==============================================================================
# N√ì 8: CANCELAMENTO (ETAPA 2 - CONFIRMAR)
# ==============================================================================

def cancelar_confirmar(state: SecretaryState):
    """
    Confirma o cancelamento do agendamento.

    Este n√≥ √© executado quando o usu√°rio fornece o ID.

    Args:
        state: Estado atual do grafo

    Returns:
        dict: Atualiza√ß√£o com 'response' da confirma√ß√£o
    """
    runtime = SecretaryRuntime(state.conversation, state.channel, state.messages_sent)

    try:
        # Extrair ID do input do usu√°rio
        appointment_id = int(state.user_input.strip())

        response_text = cancelar_agendamento(appointment_id, runtime)

        print(f"‚úÖ Agendamento {appointment_id} cancelado")

        return {"response": response_text}

    except ValueError:
        return {
            "response": "‚ùå Por favor, informe apenas o n√∫mero do ID do agendamento."
        }


# ==============================================================================
# N√ì 9: REAGENDAMENTO (ETAPA 1 - LISTAR)
# ==============================================================================

def reagendar_listar(state: SecretaryState):
    """
    Lista agendamentos e solicita ID para reagendar.

    Este √© o primeiro passo do fluxo de reagendamento.
    Atualiza 'step' para AGUARDANDO_ID_REAGENDAR.

    Args:
        state: Estado atual do grafo

    Returns:
        dict: Atualiza√ß√£o com 'response' e 'step'
    """
    runtime = SecretaryRuntime(state.conversation, state.channel, state.messages_sent)

    # Consultar agendamentos
    result = consultar_agendamentos(runtime)

    # Verificar se h√° agendamentos (detectar se a mensagem indica aus√™ncia)
    if "n√£o possui agendamentos" in result.lower() or "nenhum agendamento" in result.lower():
        # N√£o h√° agendamentos, apenas retornar a mensagem
        print(f"‚ö†Ô∏è Sem agendamentos para reagendar")
        return {"response": result}

    # H√° agendamentos, adicionar instru√ß√£o para solicitar ID
    response_text = result + "\n\n‚ùì Informe o ID da consulta que deseja reagendar."

    print(f"üìÖ Aguardando ID para reagendamento")

    return {
        "response": response_text,
        "step": "AGUARDANDO_ID_REAGENDAR"
    }


# ==============================================================================
# N√ì 10: REAGENDAMENTO (ETAPA 2 - CONFIRMAR)
# ==============================================================================

def reagendar_confirmar(state: SecretaryState):
    """
    Confirma o reagendamento e gera novo link.

    Este n√≥ √© executado quando o usu√°rio fornece o ID.

    Args:
        state: Estado atual do grafo

    Returns:
        dict: Atualiza√ß√£o com 'response' contendo novo link
    """
    runtime = SecretaryRuntime(state.conversation, state.channel, state.messages_sent)

    try:
        # Extrair ID do input do usu√°rio
        appointment_id = int(state.user_input.strip())

        response_text = reagendar_consulta(appointment_id, runtime)

        print(f"üîÑ Agendamento {appointment_id} reagendado")

        return {"response": response_text}

    except ValueError:
        return {
            "response": "‚ùå Por favor, informe apenas o n√∫mero do ID do agendamento."
        }


# ==============================================================================
# N√ì 11: CONVERSA LIVRE
# ==============================================================================

def handle_conversation(state: SecretaryState):
    """
    Gera resposta conversacional livre usando LLM.

    Este n√≥ √© executado quando a inten√ß√£o √© "OUTRO" ou quando o usu√°rio
    est√° apenas conversando sem uma inten√ß√£o espec√≠fica de a√ß√£o.

    Args:
        state: Estado atual do grafo

    Returns:
        dict: Atualiza√ß√£o com 'response' contendo a mensagem gerada
    """
    agent = state.agent
    llm = LLMFactory(agent).llm
    base_prompt = agent.build_prompt()

    # Construir hist√≥rico de conversa
    history_messages = []
    for msg in state.chat_history:
        if hasattr(msg, 'type'):
            if msg.type == 'human':
                history_messages.append(f"Usu√°rio: {msg.content}")
            elif msg.type == 'ai':
                history_messages.append(f"Assistente: {msg.content}")

    history_text = "\n".join(history_messages) if history_messages else ""

    # Construir prompt completo
    if history_text:
        prompt = f"""{base_prompt}

---

## Hist√≥rico da Conversa

{history_text}

---

## Mensagem Atual do Usu√°rio

{state.user_input}

---

Responda √† mensagem do usu√°rio de forma natural e amig√°vel."""
    else:
        # Primeira mensagem (sem hist√≥rico)
        prompt = f"""{base_prompt}

---

## Mensagem do Usu√°rio

{state.user_input}

---

Responda √† mensagem do usu√°rio de forma natural e amig√°vel."""

    # Gerar resposta
    response = llm.invoke(prompt)
    response_text = response.content.strip()

    print(f"üí¨ Resposta conversacional gerada")

    return {"response": response_text}


# ==============================================================================
# N√ì 12: ENVIAR RESPOSTA
# ==============================================================================

def send_response(state: SecretaryState):
    """
    Envia a resposta final ao usu√°rio.

    Este n√≥ √© executado ao final de cada fluxo para enviar
    a mensagem que foi preparada no campo 'response'.

    Args:
        state: Estado atual do grafo

    Returns:
        dict: Estado inalterado (final do fluxo)
    """
    runtime = SecretaryRuntime(state.conversation, state.channel, state.messages_sent)

    if state.response:
        runtime.send_message(state.response)

        # Print da resposta formatado
        print(f"\nüí¨ RESPOSTA:")
        print("‚îÄ" * 80)
        print(state.response)
        print("‚îÄ" * 80 + "\n")

    return state